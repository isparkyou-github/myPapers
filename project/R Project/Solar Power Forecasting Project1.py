# -*- coding: utf-8 -*-
"""Solar Power Forecasting Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xoQPQaKM3AvcQZGNtnb--TeHCLy6ym5p
"""

version

# Load required libraries
# install.packages("keras")
# install.packages("xgboost")
install.packages("randomForest")

# remove.packages("keras")
install.packages("keras3")
# library(keras3)

install.packages("xgboost")

install.packages("ggplot2")

# install.packages("remotes")
# remotes::install_github("dmlc/xgboost", subdir = "R-package")

library(keras3)
library(xgboost)
library(randomForest)
library(ggplot2)

getwd()

# List of dataset file paths
dataset_files <- c("/content/dataset_files/station00.csv")
# , "/content/dataset_files/station01.csv", "/content/dataset_files/station02.csv", "/content/dataset_files/station03.csv", "/content/dataset_files/station04.csv", "/content/dataset_files/station05.csv", "/content/dataset_files/station06.csv", "/content/dataset_files/station07.csv", "/content/dataset_files/station08.csv", "/content/dataset_files/station09.csv"

# Initialize empty list to store datasets
datasets <- list()

# Loop through each dataset file and load into a list
for (file in dataset_files) {
  dataset <- read.csv(file)  # Load each dataset
  datasets[[file]] <- dataset  # Store in the list
}

# Combine all datasets into one large dataset
combined_dataset <- do.call(rbind, datasets)

# Subset the data to only use 300 samples
set.seed(42)
sampled_idx <- sample(1:nrow(combined_dataset), 300)
sampled_data <- combined_dataset[sampled_idx, ]

# Select middle 13 columns as features and last column as the target variable
if (ncol(combined_dataset) < 15) stop("Not enough columns in the dataset.")
X <- combined_dataset[, 2:(ncol(combined_dataset)-1)]  # Middle 13 columns as features
y <- combined_dataset$power  # Last column 'power' as the target

# Split into 80% training and 20% testing
set.seed(42)
n <- nrow(X)
train_idx <- sample(1:n, size = 0.8 * n)
test_idx <- setdiff(1:n, train_idx)

cat("Number of rows in train_set: ", nrow(train_idx), "\n")
cat("Number of rows in test_set: ", nrow(test_idx), "\n")

# Inside the For-loop, write code so that one histogram is plotted for each column.
for(i in 2:14) {
# 提取第 i 列的列名
  column_name <- names(dataset)[i]
  # 绘制直方图
  hist(Smarket[[i]], main = paste("Histogram of", column_name), xlab = column_name)
}

mmodel <- lm (power ~ lmd_windspeed + lmd_winddirection + lmd_pressure + lmd_temperature + lmd_diffuseirrad + lmd_totalirrad + nwp_pressure + nwp_winddirection + nwp_windspeed + nwp_humidity + nwp_globalirrad + nwp_directirrad + nwp_temperature, data = train_idx)

summary(mmodel)

summary(mmodel)$coef

RSE = sigma(mmodel)  # RSE
error_rate = RSE/mean(train_set$sales)

print(RSE)
print(error_rate)

library()

library(Amelia)
library(mlbench)
missmap(mmodel, col=c("blue", "red"), legend=FALSE)

library(corrplot)

# Calculate the correlation matrix using the `cor` function
correlations <- cor(mmodel[, 2:14])#complete here
correlations

# Use the corrplot function with the shade method.
corrplot(correlations, method = "shade")

# Use the corrplot function with the pie method.
corrplot(correlations, method = "pie")

# Use the corrplot function with the circle method.
corrplot(correlations, method = "circle")

pairs(mmodel, col=mmodel$power)

library(caret)

x <- Smarket[,2:14]
y <- Smarket[,15]
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)

# Create a  Logistics Regression called glm.fit, having Direction as Dependent Variable.
# Don't forget to provide the family parameter.
glm.fit <- glm(power ~ lmd_windspeed + lmd_winddirection + lmd_pressure + lmd_temperature + lmd_diffuseirrad + lmd_totalirrad + nwp_pressure + nwp_winddirection + nwp_windspeed + nwp_humidity + nwp_globalirrad + nwp_directirrad + nwp_temperature, data = mmodel, family = binomial)#Complete here

summary(glm.fit)

glm.probs <- predict(glm.fit,type = "response")
glm.probs[1:5]

# Write your code here, use the ifelse function and 0.5 as threshold to define "Up" and "Down".
glm.pred <- ifelse(glm.probs > 0.5, "Up", "Down")#Complete here.

attach(mmodel)
table(glm.pred,Direction)

# Use the mean function to check accuracy.
mean(glm.pred == power)

X_train <- as.matrix(X[train_idx, ])
y_train <- y[train_idx]
X_test <- as.matrix(X[test_idx, ])
y_test <- y[test_idx]



head(dataset,10)

tail(dataset,10)

summary(dataset)

str(dataset)

length(dataset)

names(dataset)

colnames(dataset)

plot(dataset)

plot(dataset$power, dataset$Income, xlab = 'Power', ylab = 'Income', main = 'Age vs Income')

df$Gender <- factor(df$Gender)
df$Education <- factor(df$Education)

hist(df$power, xlab = 'Power', main = 'Histogram of Power')

# Prepare data for LSTM
X_train_lstm <- array(X_train, dim = c(nrow(X_train), ncol(X_train), 1))
X_test_lstm <- array(X_test, dim = c(nrow(X_test), ncol(X_test), 1))

# Build LSTM model - note that we explicitly pass 'input_shape' as a keyword argument
lstm_model <- keras_model_sequential()

# Add LSTM layer
lstm_model %>%
  layer_lstm(units = 10, input_shape = list(ncol(X_train), 1), return_sequences = FALSE)

# Add dense layer
lstm_model %>%
  layer_dense(units = 1)

# Compile the model
lstm_model %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)

# Train the LSTM model
lstm_model %>% fit(
  X_train_lstm, y_train, epochs = 10, batch_size = 30, verbose = 1
)

# Get predictions from the LSTM model
lstm_train_pred <- lstm_model %>% predict(X_train_lstm)
lstm_test_pred <- lstm_model %>% predict(X_test_lstm)

# Convert data to DMatrix for XGBoost
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)  # Add labels for evaluation

# Set parameters for XGBoost
xgb_params <- list(
  objective = "reg:squarederror",
  eta = 0.1,
  max_depth = 3
)

# Train XGBoost model
xgb_model <- xgb.train(params = xgb_params, data = dtrain, nrounds = 50)

# Get predictions from XGBoost
xgb_train_pred <- predict(xgb_model, X_train)
xgb_test_pred <- predict(xgb_model, X_test)

# Train Random Forest model
rf_model <- randomForest(X_train, y_train, ntree = 100)

# Get predictions from Random Forest
rf_train_pred <- predict(rf_model, X_train)
rf_test_pred <- predict(rf_model, X_test)

# Create new train and test datasets from predictions of base models
stack_train <- data.frame(
  lstm = lstm_train_pred,
  xgb = xgb_train_pred,
  rf = rf_train_pred
)

stack_test <- data.frame(
  lstm = lstm_test_pred,
  xgb = xgb_test_pred,
  rf = rf_test_pred
)

# Train a simple linear regression model as the meta-learner
meta_model <- lm(y_train ~ ., data = stack_train)

# Get final predictions using the meta-learner
final_train_pred <- predict(meta_model, stack_train)
final_test_pred <- predict(meta_model, stack_test)

# Define RMSE function
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

# Define R-squared function
r_squared <- function(actual, predictions) {
  ss_res <- sum((actual - predictions)^2)  # Residual sum of squares
  ss_tot <- sum((actual - mean(actual))^2)  # Total sum of squares
  1 - (ss_res / ss_tot)
}

# Calculate RMSE for training and testing sets
train_rmse <- rmse(y_train, final_train_pred)
test_rmse <- rmse(y_test, final_test_pred)

cat("Train RMSE: ", train_rmse, "\n")
cat("Test RMSE: ", test_rmse, "\n")

# Calculate R-squared using actual sales and predicted values
train_r_squared_value <- r_squared(y_train, final_train_pred)
test_r_squared_value <- r_squared(y_test, final_test_pred)
# Print the R-squared values
cat("Train R-squared: ", train_r_squared_value, "\n")
cat("Test R-squared: ", test_r_squared_value, "\n")

train_error_rate_test <- train_rmse/mean(y_train)
train_error_rate_test
test_error_rate_test <- test_rmse/mean(y_test)
test_error_rate_test

# Visualize the RMSE for training and testing sets
rmse_data <- data.frame(
  dataset = c("Training", "Testing"),
  rmse_value = c(train_rmse, test_rmse)
)

ggplot(rmse_data, aes(x = dataset, y = rmse_value)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(aes(label = round(rmse_value, 4)), vjust = -0.5) +
  labs(title = "RMSE Comparison", x = "Dataset", y = "Root Mean Squared Error (RMSE)") +
  theme_minimal()

# Visualizing predicted vs actual values for training set
ggplot(data.frame(actual = y_train, predicted = final_train_pred), aes(x = actual, y = predicted)) +
  geom_point(color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Predicted vs Actual (Training Set)", x = "Actual", y = "Predicted") +
  theme_minimal()

# Visualizing predicted vs actual values for testing set
ggplot(data.frame(actual = y_test, predicted = final_test_pred), aes(x = actual, y = predicted)) +
  geom_point(color = "green") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Predicted vs Actual (Testing Set)", x = "Actual", y = "Predicted") +
  theme_minimal()

